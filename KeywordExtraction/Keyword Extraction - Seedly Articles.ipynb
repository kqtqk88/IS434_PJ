{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bto-housing1.txt',\n",
       " 'bto-housing10.txt',\n",
       " 'bto-housing11.txt',\n",
       " 'bto-housing12.txt',\n",
       " 'bto-housing13.txt',\n",
       " 'bto-housing14.txt',\n",
       " 'bto-housing15.txt',\n",
       " 'bto-housing16.txt',\n",
       " 'bto-housing17.txt',\n",
       " 'bto-housing18.txt',\n",
       " 'bto-housing19.txt',\n",
       " 'bto-housing2.txt',\n",
       " 'bto-housing20.txt',\n",
       " 'bto-housing21.txt',\n",
       " 'bto-housing22.txt',\n",
       " 'bto-housing23.txt',\n",
       " 'bto-housing24.txt',\n",
       " 'bto-housing25.txt',\n",
       " 'bto-housing26.txt',\n",
       " 'bto-housing27.txt',\n",
       " 'bto-housing28.txt',\n",
       " 'bto-housing29.txt',\n",
       " 'bto-housing3.txt',\n",
       " 'bto-housing30.txt',\n",
       " 'bto-housing31.txt',\n",
       " 'bto-housing32.txt',\n",
       " 'bto-housing33.txt',\n",
       " 'bto-housing34.txt',\n",
       " 'bto-housing35.txt',\n",
       " 'bto-housing36.txt',\n",
       " 'bto-housing37.txt',\n",
       " 'bto-housing38.txt',\n",
       " 'bto-housing39.txt',\n",
       " 'bto-housing4.txt',\n",
       " 'bto-housing40.txt',\n",
       " 'bto-housing41.txt',\n",
       " 'bto-housing42.txt',\n",
       " 'bto-housing43.txt',\n",
       " 'bto-housing5.txt',\n",
       " 'bto-housing6.txt',\n",
       " 'bto-housing7.txt',\n",
       " 'bto-housing8.txt',\n",
       " 'bto-housing9.txt',\n",
       " 'comparison1.txt',\n",
       " 'comparison10.txt',\n",
       " 'comparison11.txt',\n",
       " 'comparison12.txt',\n",
       " 'comparison13.txt',\n",
       " 'comparison14.txt',\n",
       " 'comparison15.txt',\n",
       " 'comparison16.txt',\n",
       " 'comparison17.txt',\n",
       " 'comparison18.txt',\n",
       " 'comparison19.txt',\n",
       " 'comparison2.txt',\n",
       " 'comparison20.txt',\n",
       " 'comparison21.txt',\n",
       " 'comparison22.txt',\n",
       " 'comparison23.txt',\n",
       " 'comparison24.txt',\n",
       " 'comparison25.txt',\n",
       " 'comparison26.txt',\n",
       " 'comparison27.txt',\n",
       " 'comparison28.txt',\n",
       " 'comparison29.txt',\n",
       " 'comparison3.txt',\n",
       " 'comparison30.txt',\n",
       " 'comparison31.txt',\n",
       " 'comparison32.txt',\n",
       " 'comparison33.txt',\n",
       " 'comparison34.txt',\n",
       " 'comparison35.txt',\n",
       " 'comparison36.txt',\n",
       " 'comparison37.txt',\n",
       " 'comparison38.txt',\n",
       " 'comparison39.txt',\n",
       " 'comparison4.txt',\n",
       " 'comparison40.txt',\n",
       " 'comparison41.txt',\n",
       " 'comparison42.txt',\n",
       " 'comparison43.txt',\n",
       " 'comparison44.txt',\n",
       " 'comparison45.txt',\n",
       " 'comparison46.txt',\n",
       " 'comparison47.txt',\n",
       " 'comparison48.txt',\n",
       " 'comparison49.txt',\n",
       " 'comparison5.txt',\n",
       " 'comparison50.txt',\n",
       " 'comparison51.txt',\n",
       " 'comparison52.txt',\n",
       " 'comparison53.txt',\n",
       " 'comparison54.txt',\n",
       " 'comparison55.txt',\n",
       " 'comparison56.txt',\n",
       " 'comparison57.txt',\n",
       " 'comparison58.txt',\n",
       " 'comparison59.txt',\n",
       " 'comparison6.txt',\n",
       " 'comparison60.txt',\n",
       " 'comparison61.txt',\n",
       " 'comparison62.txt',\n",
       " 'comparison63.txt',\n",
       " 'comparison64.txt',\n",
       " 'comparison65.txt',\n",
       " 'comparison66.txt',\n",
       " 'comparison67.txt',\n",
       " 'comparison68.txt',\n",
       " 'comparison69.txt',\n",
       " 'comparison7.txt',\n",
       " 'comparison70.txt',\n",
       " 'comparison71.txt',\n",
       " 'comparison72.txt',\n",
       " 'comparison73.txt',\n",
       " 'comparison74.txt',\n",
       " 'comparison75.txt',\n",
       " 'comparison76.txt',\n",
       " 'comparison8.txt',\n",
       " 'comparison9.txt',\n",
       " 'financial-lifestyle1.txt',\n",
       " 'financial-lifestyle10.txt',\n",
       " 'financial-lifestyle11.txt',\n",
       " 'financial-lifestyle12.txt',\n",
       " 'financial-lifestyle13.txt',\n",
       " 'financial-lifestyle14.txt',\n",
       " 'financial-lifestyle15.txt',\n",
       " 'financial-lifestyle16.txt',\n",
       " 'financial-lifestyle17.txt',\n",
       " 'financial-lifestyle18.txt',\n",
       " 'financial-lifestyle19.txt',\n",
       " 'financial-lifestyle2.txt',\n",
       " 'financial-lifestyle20.txt',\n",
       " 'financial-lifestyle21.txt',\n",
       " 'financial-lifestyle22.txt',\n",
       " 'financial-lifestyle23.txt',\n",
       " 'financial-lifestyle24.txt',\n",
       " 'financial-lifestyle25.txt',\n",
       " 'financial-lifestyle26.txt',\n",
       " 'financial-lifestyle27.txt',\n",
       " 'financial-lifestyle28.txt',\n",
       " 'financial-lifestyle29.txt',\n",
       " 'financial-lifestyle3.txt',\n",
       " 'financial-lifestyle30.txt',\n",
       " 'financial-lifestyle4.txt',\n",
       " 'financial-lifestyle5.txt',\n",
       " 'financial-lifestyle6.txt',\n",
       " 'financial-lifestyle7.txt',\n",
       " 'financial-lifestyle8.txt',\n",
       " 'financial-lifestyle9.txt',\n",
       " 'insurance1.txt',\n",
       " 'insurance10.txt',\n",
       " 'insurance11.txt',\n",
       " 'insurance12.txt',\n",
       " 'insurance13.txt',\n",
       " 'insurance14.txt',\n",
       " 'insurance15.txt',\n",
       " 'insurance16.txt',\n",
       " 'insurance17.txt',\n",
       " 'insurance18.txt',\n",
       " 'insurance19.txt',\n",
       " 'insurance2.txt',\n",
       " 'insurance20.txt',\n",
       " 'insurance21.txt',\n",
       " 'insurance22.txt',\n",
       " 'insurance23.txt',\n",
       " 'insurance24.txt',\n",
       " 'insurance25.txt',\n",
       " 'insurance26.txt',\n",
       " 'insurance27.txt',\n",
       " 'insurance28.txt',\n",
       " 'insurance29.txt',\n",
       " 'insurance3.txt',\n",
       " 'insurance30.txt',\n",
       " 'insurance31.txt',\n",
       " 'insurance32.txt',\n",
       " 'insurance33.txt',\n",
       " 'insurance34.txt',\n",
       " 'insurance35.txt',\n",
       " 'insurance36.txt',\n",
       " 'insurance37.txt',\n",
       " 'insurance38.txt',\n",
       " 'insurance39.txt',\n",
       " 'insurance4.txt',\n",
       " 'insurance40.txt',\n",
       " 'insurance41.txt',\n",
       " 'insurance42.txt',\n",
       " 'insurance43.txt',\n",
       " 'insurance44.txt',\n",
       " 'insurance45.txt',\n",
       " 'insurance46.txt',\n",
       " 'insurance47.txt',\n",
       " 'insurance5.txt',\n",
       " 'insurance6.txt',\n",
       " 'insurance7.txt',\n",
       " 'insurance8.txt',\n",
       " 'insurance9.txt',\n",
       " 'investing1.txt',\n",
       " 'investing10.txt',\n",
       " 'investing11.txt',\n",
       " 'investing12.txt',\n",
       " 'investing13.txt',\n",
       " 'investing14.txt',\n",
       " 'investing15.txt',\n",
       " 'investing16.txt',\n",
       " 'investing17.txt',\n",
       " 'investing18.txt',\n",
       " 'investing19.txt',\n",
       " 'investing2.txt',\n",
       " 'investing20.txt',\n",
       " 'investing21.txt',\n",
       " 'investing22.txt',\n",
       " 'investing23.txt',\n",
       " 'investing24.txt',\n",
       " 'investing25.txt',\n",
       " 'investing26.txt',\n",
       " 'investing27.txt',\n",
       " 'investing28.txt',\n",
       " 'investing29.txt',\n",
       " 'investing3.txt',\n",
       " 'investing30.txt',\n",
       " 'investing4.txt',\n",
       " 'investing5.txt',\n",
       " 'investing6.txt',\n",
       " 'investing7.txt',\n",
       " 'investing8.txt',\n",
       " 'investing9.txt',\n",
       " 'personal-finance-life-stages1.txt',\n",
       " 'personal-finance-life-stages2.txt',\n",
       " 'personal-finance-life-stages3.txt',\n",
       " 'personal-finance-life-stages4.txt',\n",
       " 'personal-finance-life-stages5.txt',\n",
       " 'personal-finance-life-stages6.txt',\n",
       " 'personal-finance-life-stages7.txt',\n",
       " 'policies-opinion1.txt',\n",
       " 'policies-opinion10.txt',\n",
       " 'policies-opinion11.txt',\n",
       " 'policies-opinion12.txt',\n",
       " 'policies-opinion13.txt',\n",
       " 'policies-opinion14.txt',\n",
       " 'policies-opinion15.txt',\n",
       " 'policies-opinion16.txt',\n",
       " 'policies-opinion17.txt',\n",
       " 'policies-opinion18.txt',\n",
       " 'policies-opinion19.txt',\n",
       " 'policies-opinion2.txt',\n",
       " 'policies-opinion20.txt',\n",
       " 'policies-opinion21.txt',\n",
       " 'policies-opinion22.txt',\n",
       " 'policies-opinion23.txt',\n",
       " 'policies-opinion24.txt',\n",
       " 'policies-opinion25.txt',\n",
       " 'policies-opinion26.txt',\n",
       " 'policies-opinion27.txt',\n",
       " 'policies-opinion28.txt',\n",
       " 'policies-opinion29.txt',\n",
       " 'policies-opinion3.txt',\n",
       " 'policies-opinion30.txt',\n",
       " 'policies-opinion4.txt',\n",
       " 'policies-opinion5.txt',\n",
       " 'policies-opinion6.txt',\n",
       " 'policies-opinion7.txt',\n",
       " 'policies-opinion8.txt',\n",
       " 'policies-opinion9.txt',\n",
       " 'savings1.txt',\n",
       " 'savings10.txt',\n",
       " 'savings11.txt',\n",
       " 'savings12.txt',\n",
       " 'savings13.txt',\n",
       " 'savings14.txt',\n",
       " 'savings15.txt',\n",
       " 'savings16.txt',\n",
       " 'savings17.txt',\n",
       " 'savings18.txt',\n",
       " 'savings19.txt',\n",
       " 'savings2.txt',\n",
       " 'savings20.txt',\n",
       " 'savings21.txt',\n",
       " 'savings22.txt',\n",
       " 'savings23.txt',\n",
       " 'savings24.txt',\n",
       " 'savings25.txt',\n",
       " 'savings26.txt',\n",
       " 'savings27.txt',\n",
       " 'savings28.txt',\n",
       " 'savings29.txt',\n",
       " 'savings3.txt',\n",
       " 'savings30.txt',\n",
       " 'savings4.txt',\n",
       " 'savings5.txt',\n",
       " 'savings6.txt',\n",
       " 'savings7.txt',\n",
       " 'savings8.txt',\n",
       " 'savings9.txt',\n",
       " 'women-finance1.txt',\n",
       " 'women-finance10.txt',\n",
       " 'women-finance11.txt',\n",
       " 'women-finance12.txt',\n",
       " 'women-finance2.txt',\n",
       " 'women-finance3.txt',\n",
       " 'women-finance4.txt',\n",
       " 'women-finance5.txt',\n",
       " 'women-finance6.txt',\n",
       " 'women-finance7.txt',\n",
       " 'women-finance8.txt',\n",
       " 'women-finance9.txt']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import pandas\n",
    "\n",
    "# Get all filenames\n",
    "corpus = nltk.corpus.PlaintextCorpusReader('../WebScraper/seedly-articles/', '.+\\.txt')\n",
    "fids = corpus.fileids()\n",
    "\n",
    "# Lines and words to ignore in dataset\n",
    "stop_lines = ['< BACK TO MAIN BLOG\\n', \n",
    "              'Whether you love or hate our content... WE WANT TO HEAR WHAT YOU THINK! \\n', \n",
    "              '#wpdevar_comment_1 span,#wpdevar_comment_1 iframe{width:100% !important;} \\n']\n",
    "\n",
    "extra_stop_words = ['comments']\n",
    "\n",
    "fids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare corpus and category labels\n",
    "dataset = {}\n",
    "corpus = []\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "for fid in fids:\n",
    "    z = re.search(r'\\d', fid).span()[0]\n",
    "    category = fid[:z]\n",
    "    \n",
    "    # Process each file\n",
    "    with open('../WebScraper/seedly-articles/' + fid, encoding=\"utf8\") as file:\n",
    "        current_file = {}\n",
    "        current_file['content'] = []\n",
    "        current_file['fid'] = fid\n",
    "        \n",
    "        # Process each line and extract title, date and content of article\n",
    "        for i, line in enumerate(file):\n",
    "            if i == 5:\n",
    "                current_file['title'] = line.replace('\\n', '').strip()\n",
    "            elif i == 13:\n",
    "                current_file['date'] = line.strip()\n",
    "            elif line in stop_lines or i == 15:\n",
    "                continue\n",
    "            else:\n",
    "                # Tokenize and preprocess\n",
    "                line = line.replace(\"â€™\", '')\n",
    "                line = line.replace(\"\\n\", ' ')\n",
    "                line = line.replace(\"Guest Contributor\", '')\n",
    "                content = nltk.word_tokenize(line)\n",
    "                content = [w.lower() for w in content]\n",
    "                content = [w for w in content if re.search('^[a-z]+$', w)]\n",
    "                content = [w for w in content if w not in stop_words and w not in extra_stop_words]\n",
    "                current_file['content'].extend(content)\n",
    "        current_file['content'] = ' '.join(current_file['content'])\n",
    "        corpus.append(current_file['content'])\n",
    "    \n",
    "    # Append file id, title, date, content to dataset\n",
    "    if category not in dataset:\n",
    "        dataset[category] = [current_file]\n",
    "    else:\n",
    "        dataset[category].append(current_file)\n",
    "\n",
    "        \n",
    "# dataset: \n",
    "#   category: [{\n",
    "#     title: str,\n",
    "#     date: str,\n",
    "#     content: str\n",
    "#   },\n",
    "#   {\n",
    "#     title: str,\n",
    "#     date: str,\n",
    "#     content: str\n",
    "#   }]   \n",
    "        \n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare corpus for each category\n",
    "corpus_bto_housing = [file['content'] for file in dataset['bto-housing']]\n",
    "corpus_comparison = [file['content'] for file in dataset['comparison']]\n",
    "corpus_financial_lifestyle = [file['content'] for file in dataset['financial-lifestyle']]\n",
    "corpus_insurance = [file['content'] for file in dataset['insurance']]\n",
    "corpus_investing = [file['content'] for file in dataset['investing']]\n",
    "corpus_personal_finance_life_stages = [file['content'] for file in dataset['personal-finance-life-stages']]\n",
    "corpus_policies_opinion = [file['content'] for file in dataset['policies-opinion']]\n",
    "corpus_savings = [file['content'] for file in dataset['savings']]\n",
    "corpus_women_finance = [file['content'] for file in dataset['women-finance']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import seaborn as sns\n",
    "\n",
    "#Most frequently occuring words\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in      \n",
    "                   vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                       reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "#Most frequently occuring Bi-grams\n",
    "def get_top_n2_words(corpus, n=None):\n",
    "    vec1 = CountVectorizer(ngram_range=(2,2),  \n",
    "            max_features=2000).fit(corpus)\n",
    "    bag_of_words = vec1.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
    "                  vec1.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "#Most frequently occuring Tri-grams\n",
    "def get_top_n3_words(corpus, n=None):\n",
    "    vec1 = CountVectorizer(ngram_range=(3,3), \n",
    "           max_features=2000).fit(corpus)\n",
    "    bag_of_words = vec1.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
    "                  vec1.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                reverse=True)\n",
    "    return words_freq[:n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available corpuses:\n",
    "#\n",
    "# corpus_bto_housing\n",
    "# corpus_comparison\n",
    "# corpus_financial_lifestyle\n",
    "# corpus_insurance\n",
    "# corpus_investing\n",
    "# corpus_personal_finance_life_stages\n",
    "# corpus_policies_opinion\n",
    "# corpus_savings\n",
    "# corpus_women_finance\n",
    "\n",
    "# Choose an available corpus from above to be set to this variable\n",
    "corpus = corpus_insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "corpus_df = pd.DataFrame(corpus,columns=[\"comments\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>jacqueline yan min readinsurance nomination wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>kenneth lou min readinsurance lifes goalkeeper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>cherie tan min readaccording singapore cancer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>thaddeus tan min readever picture motorbike we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ming feng min read bringing mother japan last ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            comments\n",
       "0  jacqueline yan min readinsurance nomination wh...\n",
       "1  kenneth lou min readinsurance lifes goalkeeper...\n",
       "2  cherie tan min readaccording singapore cancer ...\n",
       "3  thaddeus tan min readever picture motorbike we...\n",
       "4  ming feng min read bringing mother japan last ..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer(max_df=0.8,stop_words=stop_words, max_features=10000, ngram_range=(1,3))\n",
    "X=cv.fit_transform(corpus)\n",
    "\n",
    "list(cv.vocabulary_.keys())[:10]\n",
    "\n",
    "#Convert most freq words to dataframe for plotting bar plot\n",
    "top_words = get_top_n_words(corpus, n=20)\n",
    "top_df = pandas.DataFrame(top_words)\n",
    "top_df.columns=[\"Word\", \"Freq\"]\n",
    "\n",
    "#Barplot of most freq words\n",
    "sns.set(rc={'figure.figsize':(13,8)})\n",
    "g = sns.barplot(x=\"Word\", y=\"Freq\", data=top_df)\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top2_words = get_top_n2_words(corpus, n=20)\n",
    "top2_df = pandas.DataFrame(top2_words)\n",
    "top2_df.columns=[\"Bi-gram\", \"Freq\"]\n",
    "print(top2_df)\n",
    "\n",
    "#Barplot of most freq Bi-grams\n",
    "sns.set(rc={'figure.figsize':(13,8)})\n",
    "h=sns.barplot(x=\"Bi-gram\", y=\"Freq\", data=top2_df)\n",
    "h.set_xticklabels(h.get_xticklabels(), rotation=45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top3_words = get_top_n3_words(corpus, n=20)\n",
    "top3_df = pandas.DataFrame(top3_words)\n",
    "top3_df.columns=[\"Tri-gram\", \"Freq\"]\n",
    "print(top3_df)\n",
    "#Barplot of most freq Tri-grams\n",
    "sns.set(rc={'figure.figsize':(13,8)})\n",
    "j=sns.barplot(x=\"Tri-gram\", y=\"Freq\", data=top3_df)\n",
    "j.set_xticklabels(j.get_xticklabels(), rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    " \n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(X)\n",
    "# get feature names\n",
    "feature_names=cv.get_feature_names()\n",
    " \n",
    "# fetch document for which keywords needs to be extracted\n",
    "doc=' '.join(corpus)\n",
    " \n",
    "#generate tf-idf for the given document\n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for sorting tf_idf in descending order\n",
    "from scipy.sparse import coo_matrix\n",
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    " \n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    " \n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    " \n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results\n",
    "#sort the tf-idf vectors by descending order of scores\n",
    "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "#extract only the top n; n here is 10\n",
    "keywords=extract_topn_from_vector(feature_names,sorted_items,5)\n",
    " \n",
    "# now print the results\n",
    "print(\"\\nAbstract:\")\n",
    "print(doc)\n",
    "print(\"\\nKeywords:\")\n",
    "for k in keywords:\n",
    "    print(k,keywords[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
